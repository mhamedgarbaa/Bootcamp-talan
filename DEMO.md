# ğŸ¯ Guide de DÃ©monstration - E-Commerce Analytics Platform

> **DurÃ©e estimÃ©e : 15-20 minutes**
> Ce guide vous accompagne Ã  travers une dÃ©mo complÃ¨te du projet.

---

## ğŸ”§ PrÃ©-requis (avant la dÃ©mo)

```bash
# 1. Lancer Docker Desktop
# 2. DÃ©marrer tous les services
docker-compose up -d

# 3. Charger les donnÃ©es
python ingestion/load_olist_simple.py

# 4. Lancer le dashboard
streamlit run visualization/streamlit_app.py --server.port 8501
```

---

## ğŸ“Œ PARTIE 1 : PrÃ©sentation de l'Architecture (2 min)

### Montrer le `docker-compose.yml`
- **11 services** conteneurisÃ©s
- Architecture **polyglot persistence** : chaque base de donnÃ©es est choisie pour ses forces

| Base | ModÃ¨le | Cas d'usage |
|------|--------|-------------|
| MongoDB | Document | Produits avec schÃ©ma flexible, avis textuels |
| ClickHouse | Colonnes (OLAP) | AgrÃ©gations rapides sur millions de lignes de ventes |
| Neo4j | Graphe | Relations clients â†’ produits, recommandations |

### Montrer les containers actifs
```bash
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

---

## ğŸ“Œ PARTIE 2 : Le Dataset Olist (2 min)

Ouvrir un terminal et montrer :
```bash
# 9 fichiers CSV, ~120 MB, dataset rÃ©el brÃ©silien
dir data\raw\*.csv

# 99 441 commandes, 32 951 produits, 3 095 vendeurs
# PÃ©riode : 2016-2018
```

**Point clÃ©** : Ce n'est pas un dataset synthÃ©tique â€” c'est un vrai dataset e-commerce publiÃ© par Olist sur Kaggle.

---

## ğŸ“Œ PARTIE 3 : MongoDB - Document Store (3 min)

### Ouvrir Mongo Express : http://localhost:8091

Montrer :
1. La base `ecommerce` avec les collections `products` et `reviews`
2. Cliquer sur un document produit â†’ schÃ©ma flexible JSON
3. Cliquer sur un avis â†’ texte libre + score

### Ou via le terminal :
```bash
docker exec bootcamptalan-mongodb-1 mongosh -u admin -p password123 --authenticationDatabase admin --eval "
  db = db.getSiblingDB('ecommerce');
  print('=== Produits ===');
  printjson(db.products.findOne());
  print('\n=== Avis clients ===');
  printjson(db.reviews.findOne());
  print('\n=== Stats ===');
  print('Produits: ' + db.products.countDocuments());
  print('Avis: ' + db.reviews.countDocuments());
"
```

**Point clÃ©** : MongoDB est idÃ©al pour les documents semi-structurÃ©s. Un produit peut avoir des attributs variables selon sa catÃ©gorie.

---

## ğŸ“Œ PARTIE 4 : ClickHouse - Analytics OLAP (3 min)

### RequÃªtes analytiques ultra-rapides

```bash
# Top 10 des catÃ©gories par chiffre d'affaires
docker exec bootcamptalan-clickhouse-1 clickhouse-client --query "
  SELECT 
    product_category AS category,
    count() AS nb_ventes,
    round(sum(price), 2) AS chiffre_affaires,
    round(avg(price), 2) AS panier_moyen
  FROM ecommerce.sales_fact
  WHERE product_category != 'unknown'
  GROUP BY category
  ORDER BY chiffre_affaires DESC
  LIMIT 10
  FORMAT PrettyCompact
"

# Ventes par mois (time series)
docker exec bootcamptalan-clickhouse-1 clickhouse-client --query "
  SELECT 
    toYYYYMM(order_purchase_timestamp) AS mois,
    count() AS nb_commandes,
    round(sum(price), 2) AS revenue
  FROM ecommerce.sales_fact
  GROUP BY mois
  ORDER BY mois
  FORMAT PrettyCompact
"

# RÃ©partition gÃ©ographique (par Ã©tat brÃ©silien)
docker exec bootcamptalan-clickhouse-1 clickhouse-client --query "
  SELECT 
    customer_state AS etat,
    count() AS nb_commandes,
    round(avg(review_score), 2) AS note_moyenne,
    round(sum(price), 2) AS chiffre_affaires
  FROM ecommerce.sales_fact
  WHERE customer_state != 'unknown'
  GROUP BY etat
  ORDER BY nb_commandes DESC
  LIMIT 10
  FORMAT PrettyCompact
"

# KPIs globaux
docker exec bootcamptalan-clickhouse-1 clickhouse-client --query "
  SELECT 
    count() AS total_ventes,
    round(sum(price), 2) AS ca_total,
    round(avg(price), 2) AS prix_moyen,
    uniq(customer_id) AS clients_uniques,
    round(avg(review_score), 2) AS note_moyenne
  FROM ecommerce.sales_fact
  FORMAT PrettyCompact
"
```

**Point clÃ©** : ClickHouse exÃ©cute ces agrÃ©gations en **millisecondes** vs secondes pour un RDBMS classique. ConÃ§u pour l'analytics.

---

## ğŸ“Œ PARTIE 5 : Neo4j - Graph Database (3 min)

### Ouvrir Neo4j Browser : http://localhost:7474
- Login : `neo4j` / `password123`

### RequÃªtes visuelles (copier dans le Neo4j Browser) :

```cypher
// Visualiser le graphe : Clients qui ont achetÃ© des produits
MATCH (c:Customer)-[r:PURCHASED]->(p:Product)
RETURN c, r, p LIMIT 50
```

```cypher
// Clients qui ont achetÃ© le mÃªme produit (base pour recommandations)
MATCH (c1:Customer)-[:PURCHASED]->(p:Product)<-[:PURCHASED]-(c2:Customer)
WHERE c1 <> c2
RETURN c1.customer_id AS client1, c2.customer_id AS client2, 
       p.product_id AS produit_commun
LIMIT 20
```

```cypher
// Stats du graphe
MATCH (n) RETURN labels(n)[0] AS type, count(n) AS nombre
```

**Point clÃ©** : Neo4j permet de traverser les relations en O(1), idÃ©al pour les recommandations et l'analyse de rÃ©seau.

---

## ğŸ“Œ PARTIE 6 : Apache Airflow - Orchestration (2 min)

### Ouvrir Airflow : http://localhost:8080
- Login : `admin` / `admin`

Montrer :
1. Le DAG `ecommerce_etl` avec ses **9 tÃ¢ches**
2. Le graphe de dÃ©pendances (vue Graph)
3. Les phases : **Extract â†’ Transform â†’ Load â†’ Spark ELT â†’ Verify**

### Les 9 tÃ¢ches du pipeline :
| # | TÃ¢che | Description |
|---|-------|-------------|
| 1 | `extract_validate` | VÃ©rification des CSV bruts |
| 2 | `transform_clean` | Nettoyage, dÃ©duplication, types |
| 3 | `load_mongodb` | Chargement documents |
| 4 | `load_clickhouse` | Chargement OLAP |
| 5 | `load_neo4j` | Chargement graphe |
| 6 | `spark_batch_analytics` | AgrÃ©gations Spark (4 parquets) |
| 7 | `spark_ml_recommendations` | Reco ALS (Spark MLlib) |
| 8 | `verify_mongodb` | VÃ©rification post-load |
| 9 | `verify_clickhouse` | VÃ©rification post-load |

**Point clÃ©** : Airflow permet de planifier, monitorer et relancer automatiquement chaque Ã©tape du pipeline.

---

## ğŸ“Œ PARTIE 7 : Apache Spark - Traitement batch & ML (2 min)

### Spark UI : http://localhost:8082

Montrer les jobs Spark exÃ©cutÃ©s.

### Ce que Spark produit :

```bash
# 4 fichiers Parquet d'analytics :
# - sales_by_state/     â†’ Ventes par Ã©tat brÃ©silien
# - sales_by_month/     â†’ Ã‰volution temporelle
# - product_by_category/â†’ Performance par catÃ©gorie
# - customer_segments/  â†’ Segmentation RFM (RÃ©cence, FrÃ©quence, Montant)

# + 1 fichier ML :
# - recommendations/    â†’ Recommandations produits (algorithme ALS)
```

**Point clÃ©** : Spark traite le dataset complet en mÃ©moire et produit des rÃ©sultats Parquet optimisÃ©s.

---

## ğŸ“Œ PARTIE 8 : Dashboard Streamlit (2 min)

### Ouvrir : http://localhost:8501

Le dashboard affiche :
- ğŸ“Š KPIs principaux (total commandes, CA, panier moyen)
- ğŸ“ˆ Graphique des ventes par mois
- ğŸ—ºï¸ RÃ©partition gÃ©ographique par Ã©tat
- ğŸ“¦ Top catÃ©gories produits
- â­ Distribution des notes clients

---

## ğŸ“Œ PARTIE 9 : MinIO - Data Lake S3 (1 min)

### Ouvrir : http://localhost:9001
- Login : `minioadmin` / `minioadmin`

Montrer que MinIO est prÃªt comme data lake local compatible S3 pour stocker les fichiers Parquet.

---

## ğŸ¤ Points clÃ©s pour la Q&A

1. **Pourquoi polyglot persistence ?** â†’ Chaque base excelle dans son domaine. MongoDB pour la flexibilitÃ©, ClickHouse pour la vitesse analytique, Neo4j pour les relations.

2. **Pourquoi Airflow ?** â†’ Orchestration avec retry, monitoring, scheduling. Production-ready.

3. **Pourquoi Spark ?** â†’ Traitement distribuÃ© scalable. Passe de 100K Ã  des millions de lignes sans changer le code.

4. **Pourquoi Docker ?** â†’ ReproductibilitÃ©. Un seul `docker-compose up` et tout est prÃªt.

5. **Dataset rÃ©el** â†’ Pas de donnÃ©es synthÃ©tiques, vrai dataset e-commerce brÃ©silien.

---

## ğŸ”— Liens rapides pendant la dÃ©mo

| Service | URL |
|---------|-----|
| Mongo Express | http://localhost:8091 |
| Neo4j Browser | http://localhost:7474 |
| Airflow | http://localhost:8080 |
| Spark Master UI | http://localhost:8082 |
| Streamlit Dashboard | http://localhost:8501 |
| MinIO Console | http://localhost:9001 |
| ClickHouse HTTP | http://localhost:8123 |
